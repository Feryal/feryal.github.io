<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Reinforcement Learning on Feryal Behbahani</title>
    <link>https://feryal.github.io/tags/deep-reinforcement-learning/</link>
    <description>Recent content in Deep Reinforcement Learning on Feryal Behbahani</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Feryal Behbahani</copyright>
    <lastBuildDate>Fri, 23 Mar 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/deep-reinforcement-learning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Successor Representations</title>
      <link>https://feryal.github.io/post/successorrepresentations/</link>
      <pubDate>Fri, 23 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://feryal.github.io/post/successorrepresentations/</guid>
      <description>

&lt;p&gt;Successor representations were introduced by &lt;a href=&#34;#references&#34;&gt;Dayan in 1993&lt;/a&gt;, as a way to
represent states by thinking of how &amp;ldquo;similarity&amp;rdquo; for TD learning is similar
to the temporal sequence of states that can be reached from a given state.&lt;/p&gt;

&lt;p&gt;Dayan derived it in the tabular case, but let&amp;rsquo;s do it when assuming a feature
vector $\phi$.&lt;/p&gt;

&lt;p&gt;We assumes that the reward function can be factorised linearly:
$$r(s) = \phi(s) \cdot w$$&lt;/p&gt;

&lt;p&gt;This can then be inserted back into the value expectation formula:
$$\begin{array}{rl}
Q^\pi(s, a) &amp;amp;= E \left\lbrack \sum_{t=0}^\infty \gamma^t R(s_t) ~|~ s_o = s, a_0 = a \right\rbrack  \\&lt;br /&gt;
 &amp;amp; = E\left\lbrack \sum_t^\infty \gamma^t \phi(s_t) \cdot w ~|~ s_0=s, a_0 = a \right\rbrack \\&lt;br /&gt;
 &amp;amp; = E\left\lbrack \sum_t^\infty \gamma^t \phi(s_t) ~|~ s_0=s, a_0 = a \right\rbrack \cdot w \\&lt;br /&gt;
 &amp;amp; = \psi^\pi(s, a) \cdot w
 \end{array} $$&lt;/p&gt;

&lt;p&gt;$\psi^\pi$ represents the Successor features, and does not depend on the rewards.
It also represents a partial model of the world / occupancy of the states given the behaviour policy.&lt;/p&gt;

&lt;p&gt;$w$ represents the rewards or the goal, and can be thought of as preferences about the given features $\phi(s)$ in a
given task.&lt;/p&gt;

&lt;p&gt;Assuming that the features $\phi(s)$ are good and consistent across a domain, we can imagine transfering to new
tasks really quickly, as we simply need to learn new goal vectors $w$.&lt;/p&gt;

&lt;p&gt;In Dayan&amp;rsquo;s case, the features $\phi(s)$ were directly the one-hot occupancy of a tabular state-space. This means
that the successor features directly corresponds to the visitation counts under a policy.
This is not as simple in more complex scenarios or when we want to learn $\phi$.&lt;/p&gt;

&lt;p&gt;One important observation, also mentioned in passing by Dayan, is that one can rewrite the definition of the
successor features $\psi$ using the Value expectation formula, obtaining independent Bellman updates for every components of $\phi$:&lt;/p&gt;

&lt;p&gt;$$E\left\lbrack \sum_t^\infty \gamma^t \phi_i(s_t) ~|~ s_0=s, a_0 = a \right\rbrack$$&lt;/p&gt;

&lt;p&gt;$$= E\left\lbrack \phi_i(s_t) + \gamma \psi_i(s_{t+1}, a^\star) - \psi_i(s_t, a_t) ~|~ s_0=s, a_0 = a \right\rbrack $$&lt;/p&gt;

&lt;p&gt;In other terms, the features $\phi(s)$ behave as the rewards in a Bellman update, where $\psi(s, a)$ takes the role
of the Q-values.&lt;/p&gt;

&lt;!-- ## Deep successor features

* Tejas
* Barreto
* Bengio

## Subgoal discovery using successor features

## Relation to Expected Features in Inverse Reinforcement Learning --&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Peter Dayan, &amp;ldquo;Improving generalization for temporal difference learning: The successor representation&amp;rdquo;, Neural Computation, 5(4):613–624, 1993. &lt;a href=&#34;http://www.gatsby.ucl.ac.uk/~dayan/papers/d93b.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- config.toml   | page preamble  | highlighting enabled for page?
--------------|----------------|-------------------------------
unset or true | unset or true  | yes
unset or true | false          | no
false         | unset or false | no
false         | true           | yes

### Imitation Learning


#### Behaviour cloning
#### Inverse Reinforcement Learning
#### Third-person Imitation Learning

### Environments

- Mujoco
- Gazeebo
- Bullet
- V-REP

### Tasks

- sim2real / darla / levine

- openai blocks

- pieter abbeel/ chelsea/ levine

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Emphasise these are only a few example tasl setups.&lt;/p&gt;

&lt;/div&gt;


$$\left [ – \frac{\hbar^2}{2 m} \frac{\partial^2}{\partial x^2} + V \right ] \Psi = i \hbar \frac{\partial}{\partial t} \Psi$$

### References

- **Transfer Learning**:

    Gupta, Abhishek, et al. [Learning invariant feature spaces to transfer skills with reinforcement learning.] (https://arxiv.org/abs/1703.02949) arXiv preprint arXiv:1703.02949, 2017 ([video](https://www.youtube.com/watch?v=NK5OkvZe_K4),[pdf](https://arxiv.org/pdf/1703.02949.pdf)).


&lt;aside class=&#34;admonition note&#34;&gt;
	&lt;div class=&#34;note-icon&#34;&gt;
		
	&lt;/div&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;Before you begin writing your content in markdown, Blackfriday has a known issue &lt;a href=&#34;https://github.com/russross/blackfriday/issues/329&#34;&gt;(#329)&lt;/a&gt; with handling deeply nested lists. Luckily, there is an easy workaround. Use 4-spaces (i.e., &lt;kbd&gt;tab&lt;/kbd&gt;) rather than 2-space indentations.&lt;/p&gt;
&lt;/div&gt;
&lt;/aside&gt;

&lt;!-- &lt;script src=&#34;//gist.github.com/spf13/7896402.js&#34;&gt;&lt;/script&gt; --&gt;

&lt;!-- &lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Hugo 0.24 Released: Big archetype update + &lt;a href=&#34;https://twitter.com/Netlify?ref_src=twsrc%5Etfw&#34;&gt;@Netlify&lt;/a&gt; _redirects etc. file support&lt;a href=&#34;https://t.co/X94FmYDEZJ&#34;&gt;https://t.co/X94FmYDEZJ&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/gohugo?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#gohugo&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/golang?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#golang&lt;/a&gt; &lt;a href=&#34;https://twitter.com/spf13?ref_src=twsrc%5Etfw&#34;&gt;@spf13&lt;/a&gt; &lt;a href=&#34;https://twitter.com/bepsays?ref_src=twsrc%5Etfw&#34;&gt;@bepsays&lt;/a&gt;&lt;/p&gt;&amp;mdash; GoHugo.io (@GoHugoIO) &lt;a href=&#34;https://twitter.com/GoHugoIO/status/877500564405444608?ref_src=twsrc%5Etfw&#34;&gt;June 21, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
  --&gt;

&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/w7Ft2ymGmfc?autoplay=1&#34; 
  style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;

&lt;!-- &lt;script async class=&#39;speakerdeck-embed&#39; data-id=&#39;4e8126e72d853c0060001f97&#39; data-ratio=&#39;1.33333333333333&#39; src=&#39;//speakerdeck.com/assets/embed.js&#39;&gt;&lt;/script&gt; --&gt;
</description>
    </item>
    
  </channel>
</rss>
